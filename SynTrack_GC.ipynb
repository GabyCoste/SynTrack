{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile as tiff\n",
    "from skimage.measure import regionprops\n",
    "import networkx as nx\n",
    "from scipy.stats import geom, multivariate_normal\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.spatial import cKDTree, KDTree\n",
    "import pandas as pd\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_seg_file = \"F08_1_roi1_registered_segmentation_CROP.tif\" #Gaby file to run here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_node_label(node):\n",
    "    \"\"\"Convert tuples like ('copy', 1, 0) to 'copy_1_0' for LEMON compatibility.\"\"\"\n",
    "    if isinstance(node, tuple):\n",
    "        return '_'.join(str(part) for part in node)\n",
    "    return str(node)\n",
    "\n",
    "def convert_tracking_graph_to_lemon(G, output_filename=\"solver/graph.lgf\"):\n",
    "    nodes = list(G.nodes)\n",
    "    nodes = [sanitize_node_label(node) for node in nodes]\n",
    "\n",
    "    arcs = []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        u_str = sanitize_node_label(u)\n",
    "        v_str = sanitize_node_label(v)\n",
    "        label = data.get('label', f\"{u_str}_{v_str}\")\n",
    "        capacity = data.get('capacity')\n",
    "        cost = data.get('weight')\n",
    "        arcs.append((u_str, v_str, label, capacity, cost))\n",
    "\n",
    "    lines = []\n",
    "    lines.append(\"@nodes\\nlabel\")\n",
    "    lines.extend(nodes)\n",
    "\n",
    "    lines.append(\"\\n@arcs\\nlabel capacity cost\")\n",
    "    for u, v, label, capacity, cost in arcs:\n",
    "        lines.append(f\"{u} {v} {label} {capacity} {cost}\")\n",
    "\n",
    "    lines.append(\"\\n@attributes\")\n",
    "    #source = sanitize_node_label(G.graph.get('source', 'unknown_source'))\n",
    "    lines.append(f\"source {nodes[0]}\")\n",
    "    lines.append(f\"target {nodes[-1]}\")\n",
    "    #lines.append(f\"target {target}\")\n",
    "\n",
    "    lemon_text = \"\\n\".join(lines)\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        f.write(lemon_text)\n",
    "\n",
    "def str_to_tuple(s):\n",
    "    \"\"\"Convert node string like 'copy_1_1' or '2_0' to tuple.\"\"\"\n",
    "    parts = s.split('_')\n",
    "    if parts[0] == 'copy':\n",
    "        return ('copy', int(parts[1]), int(parts[2]))\n",
    "    else:\n",
    "        return tuple(map(int, parts))\n",
    "\n",
    "def parse_output_txt(filename):\n",
    "    G = nx.DiGraph()\n",
    "    flowDict = {}\n",
    "    flowCost = None\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    node_section = False\n",
    "    arc_section = False\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        if line.startswith(\"Total cost:\"):\n",
    "            flowCost = float(line.split(\":\")[1].strip())\n",
    "        elif line == \"@nodes\":\n",
    "            node_section = True\n",
    "            arc_section = False\n",
    "            continue\n",
    "        elif line == \"@arcs\":\n",
    "            node_section = False\n",
    "            arc_section = True\n",
    "            continue\n",
    "        elif line.startswith(\"@attributes\"):\n",
    "            node_section = False\n",
    "            arc_section = False\n",
    "            continue\n",
    "\n",
    "        # Parse nodes\n",
    "        if node_section:\n",
    "            if line and not line.startswith(\"label\"):\n",
    "                node = str_to_tuple(line)\n",
    "                G.add_node(node)\n",
    "\n",
    "        # Parse arcs and flow\n",
    "        if arc_section:\n",
    "            if line and not line.startswith(\"label\") and not line.startswith(\"flow\"):\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 6:\n",
    "                    u_str, v_str = parts[0], parts[1]\n",
    "                    label = parts[2]\n",
    "                    capacity = int(parts[3])\n",
    "                    cost = int(parts[4])\n",
    "                    flow = int(parts[5])\n",
    "\n",
    "                    u, v = str_to_tuple(u_str), str_to_tuple(v_str)\n",
    "                    G.add_edge(u, v, capacity=capacity, cost=cost)\n",
    "\n",
    "                    if flow > 0:\n",
    "                        if u not in flowDict:\n",
    "                            flowDict[u] = {}\n",
    "                        flowDict[u][v] = flow\n",
    "\n",
    "    return flowCost, flowDict\n",
    "\n",
    "def assign_track_ids_to_segmentation(seg_img, tracks_df, scales, distance_thresh=0.001):\n",
    "    \"\"\"\n",
    "    Match regionprops in segmentation to tracks based on centroid proximity.\n",
    "    \"\"\"\n",
    "    T = seg_img.shape[0]\n",
    "    track_ids_used = set(tracks_df['track_id'])\n",
    "    max_track_id = max(track_ids_used)\n",
    "    counter = 1  # for new IDs\n",
    "\n",
    "    # Precompute the scaling matrix\n",
    "    scale_matrix = np.diag(scales)\n",
    "\n",
    "    # Create a dictionary of tracks by timepoint\n",
    "    tracks_by_time = {t + 1: tracks_df[tracks_df['t'] == t + 1] for t in range(T)}\n",
    "\n",
    "    # Initialize an empty segmentation map\n",
    "    updated_seg = np.zeros_like(seg_img, dtype=np.int32)\n",
    "\n",
    "    for t in tqdm(range(T), desc=\"Processing frames\"):\n",
    "        seg_frame = seg_img[t]\n",
    "        regions = regionprops(seg_frame.astype(int))\n",
    "\n",
    "        # Extract tracks for the current timepoint\n",
    "        tracks_t = tracks_by_time[t + 1]\n",
    "\n",
    "        # Create a KDTree for fast nearest neighbor search\n",
    "        track_positions = tracks_t[['x', 'y', 'z']].values\n",
    "        track_tree = KDTree(track_positions)\n",
    "\n",
    "        for region in regions:\n",
    "            centroid_voxel = np.array(region.centroid)\n",
    "            centroid_phys = scale_matrix @ centroid_voxel\n",
    "\n",
    "            # Find the nearest track using KDTree\n",
    "            dist, idx = track_tree.query(centroid_phys, distance_upper_bound=distance_thresh)\n",
    "\n",
    "            if dist < distance_thresh:\n",
    "                track_id = int(tracks_t.iloc[idx]['track_id'])\n",
    "            else:\n",
    "                track_id = max_track_id + counter\n",
    "                counter += 1\n",
    "\n",
    "            # Assign the track ID to the region's pixels\n",
    "            coords = region.coords  # voxel coordinates (z, y, x)\n",
    "            for coord in coords:\n",
    "                updated_seg[t, coord[0], coord[1], coord[2]] = track_id\n",
    "\n",
    "    return updated_seg\n",
    "\n",
    "def get_P(seg_I0, scales): # standard\n",
    "    \"\"\" Will return physical coordinates \"\"\"\n",
    "    regions = regionprops(seg_I0.astype(int))\n",
    "\n",
    "    # Extract centroids and store as 2xK matrix\n",
    "    centroids = np.array([region.centroid for region in regions]).T  # Transpose to get 2xK matrix\n",
    "    labels = np.array([region.label for region in regions])\n",
    "    #random.shuffle(labels)\n",
    "    #labels = np.random.randint(1000, size=len(labels))\n",
    "    #print(labels\n",
    "    # return random labels\n",
    "    # then our metrics should become much smaller\n",
    "\n",
    "    return np.diag(scales)@centroids, labels\n",
    "\n",
    "def get_P_list(seg_I0, scales): #standard\n",
    "    \"\"\" Will return physical coordinates \"\"\"\n",
    "    regions = regionprops(seg_I0.astype(int))\n",
    "\n",
    "    # Extract centroids and store as 2xK matrix\n",
    "    centroids = [np.diag(scales)@region.centroid for region in regions]  # Transpose to get 2xK matrix\n",
    "    labels = np.array([region.label for region in regions])\n",
    "    #random.shuffle(labels)\n",
    "    #labels = np.random.randint(1000, size=len(labels))\n",
    "    #print(labels\n",
    "    # return random labels\n",
    "    # then our metrics should become much smaller\n",
    "\n",
    "    return centroids, labels\n",
    "\n",
    "def dist(p1, p2, sigma): #isotropic mahalanobis distance\n",
    "    return (np.linalg.norm(p2-p1)**2)/(2*sigma**2)\n",
    "\n",
    "def spatiotemporal_dist(delta_x, delta_t, cov, p_success=0.8):\n",
    "    log_p_t = geom.logpmf(delta_t, p_success)\n",
    "    log_p_x = multivariate_normal(np.zeros(3), cov).logpdf(delta_x)\n",
    "    #log_p_x = multivariate_normal(mean=np.zeros(3), cov=np.diag([0.3**2, 0.3**2, 0.5**2])).logpdf(delta_x)\n",
    "    #return -(log_p_t + log_p_x)\n",
    "    return -(log_p_x)\n",
    "    #return (np.linalg.norm(delta_x)**2)/(2*sigma**2)\n",
    "    #return (np.linalg.norm(p2-p1)**2)/(2*sigma**2)\n",
    "\n",
    "def get_source_edges(d): # verified\n",
    "    return [((0, 0), (t, j)) for t in range(1, len(d) - 1) for j in range(len(d[t]))]\n",
    "\n",
    "def get_sink_edges(d): #verified\n",
    "    return [(('copy', t, j), (len(d) - 1, 0)) for t in range(1, len(d) - 1) for j in range(len(d[t]))]\n",
    "\n",
    "def get_temporal_edges(d, w, thresh, cov): # mahalanobis version\n",
    "    e = []\n",
    "    cov_inv = np.linalg.inv(cov)\n",
    "    cov_inv_sqrt = sqrtm(cov_inv)\n",
    "    #L = np.linalg.cholesky(cov_inv)  # or use scipy.linalg.sqrtm(cov_inv) if not PSD\n",
    "\n",
    "    for t in range(1, len(d) - 2):\n",
    "        for t_plus in range(t + 1, min(t + 1 + w, len(d) - 1)):\n",
    "            d_t = np.array(d[t])\n",
    "            d_t_plus = np.array(d[t_plus])\n",
    "\n",
    "            # Transform data to latent space\n",
    "            z_t = np.transpose(cov_inv_sqrt @ np.transpose(d_t)) \n",
    "            z_t_plus = np.transpose(cov_inv_sqrt @ np.transpose(d_t_plus))\n",
    "\n",
    "            tree = cKDTree(z_t_plus)\n",
    "            pairs = tree.query_ball_point(z_t, r=3)\n",
    "\n",
    "            for i, neighbors in enumerate(pairs):\n",
    "                for j in neighbors:\n",
    "                    #print(t, i,t_plus, j)\n",
    "                    displacement = d_t_plus[j] - d_t[i]\n",
    "                    dist = spatiotemporal_dist(displacement, t_plus - t, cov)\n",
    "                    e.append((('copy', t, i), (t_plus, j), dist))\n",
    "\n",
    "    return e\n",
    "\n",
    "def build_graph(detections, k, thresh, cov):\n",
    "    d = [[(0, 0)]] + detections + [[(0, 0)]] # we add two extra, that's why we do len(d)-1 while finding sink node\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Source node\n",
    "    G.add_node((0, 0))\n",
    "\n",
    "    # Add all original and copy nodes\n",
    "    for t in range(1, len(d)-1): # skip source and sink\n",
    "        for i in range(len(d[t])): # \n",
    "            orig = (t, i)\n",
    "            copy = ('copy', t, i)\n",
    "            G.add_node(orig, pos=d[t][i])\n",
    "            G.add_node(copy, pos=d[t][i])\n",
    "\n",
    "    # Sink node\n",
    "    G.add_node((len(d) - 1, 0))\n",
    "\n",
    "    # Source edges\n",
    "    for u, v in get_source_edges(d):\n",
    "        G.add_edge(u, v, capacity=int(1), weight=0)\n",
    "\n",
    "    # Loop edges to copies (encode detection cost here)\n",
    "    for t in range(1, len(d) - 1):  # skip source/sink\n",
    "        for i in range(len(d[t])):\n",
    "            orig = (t, i)\n",
    "            copy = ('copy', t, i)\n",
    "            G.add_edge(orig, copy, capacity=int(1), weight=-int(1000000))  # replace weight with -log(conf) if needed\n",
    "\n",
    "    # Temporal edges: from copy â†’ next original node\n",
    "    for u, v, cost in get_temporal_edges(d, 8, thresh, cov):\n",
    "        #print(cost)\n",
    "        G.add_edge(u, v, capacity=int(1), weight=int(cost*10000))\n",
    "\n",
    "    # Sink edges: from copy nodes\n",
    "    for u, v in get_sink_edges(d):\n",
    "        G.add_edge(u, v, capacity=int(1), weight=int(0))\n",
    "\n",
    "    # Node demands\n",
    "    nx.set_node_attributes(G, int(0), \"demand\") # set demand of 0 for all nodes\n",
    "    G.nodes[(0, 0)][\"demand\"] = -int(k) # make sure integer flow of -k units\n",
    "    G.nodes[(len(d) - 1, 0)][\"demand\"] = int(k) #make sure integer flow of +k units, so all flow ends up in sink\n",
    "\n",
    "    return G, d\n",
    "\n",
    "def extract_flow_paths(flowdict, source, sink, min_flow=1):\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Build a graph where edges with flow >= min_flow are included\n",
    "    for u, neighbors in flowdict.items():\n",
    "        for v, flow in neighbors.items():\n",
    "            if flow >= min_flow:\n",
    "                G.add_edge(u, v, flow=flow)\n",
    "\n",
    "    # Find all simple paths from source to sink in this flow graph\n",
    "    paths = list(nx.all_simple_paths(G, source=source, target=sink))\n",
    "    return paths\n",
    "\n",
    "def get_tracks(paths, detections): # verified\n",
    "    tracks = []\n",
    " \n",
    "    for i, path in enumerate(paths, 1):\n",
    "        #print(\"path1\")\n",
    "        for node in path[1:-1]: # first and last are edges involving source and sink so skip\n",
    "            if node[0] != 'copy':\n",
    "                t, j = node\n",
    "                #print(t, j)\n",
    "                tracks.append((t, i, detections[t-1][j]))\n",
    "\n",
    "    tracks = pd.DataFrame(tracks, columns=['t', 'track_id', 'position'])\n",
    "\n",
    "    # Expand position into separate columns\n",
    "    pos_df = tracks['position'].apply(pd.Series)\n",
    "    pos_cols = ['x', 'y', 'z'][:pos_df.shape[1]]\n",
    "    pos_df.columns = pos_cols\n",
    "\n",
    "    tracks = pd.concat([tracks.drop(columns='position'), pos_df], axis=1)\n",
    "\n",
    "    return tracks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Initial Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spatial_thresholds = [1.13]#np.linspace(0.1, 3, 100) # very large spatial threshold\n",
    "spatial_thresholds = [1000]\n",
    "scales = [0.096, 0.096, 0.33]\n",
    "\n",
    "#cov = np.diag([3, 3, 3])\n",
    "\n",
    "cov_fixed = np.array([[0.14984745, 0.01853253, 0.0749335 ],\n",
    " [0.01853253, 0.08066656, 0.02944603],\n",
    " [0.0749335,  0.02944603, 0.23925671]])\n",
    "\n",
    "cov_G_labelled = np.array([\n",
    "    [0.07384764, 0.01238046, 0.05048932],\n",
    "    [0.01238046, 0.05005508, 0.02076806],\n",
    "    [0.05048932, 0.02076806, 0.26948054]\n",
    "])\n",
    "\n",
    "cov_A_labelled = np.array([\n",
    "    [0.08763301, 0.01724761, 0.05815704],\n",
    "    [0.01724761, 0.05762029, 0.01916235],\n",
    "    [0.05815704, 0.01916235, 0.31619810]\n",
    "])\n",
    "\n",
    "covs = [cov_G_labelled]\n",
    "#covs = [cov_fixed]\n",
    "sigma_est2 = 0.13\n",
    "#covs = [np.diag([sigma_est2, sigma_est2, sigma_est2])]\n",
    "#covs = [np.eye(cov_fixed.shape[0])]\n",
    "\n",
    "scale_factor=0.33/0.096\n",
    "z_size = 160 # GABY THOUGHT: WHY 160 Why not read the x,y,z sizes to match the image? \n",
    "x_size = int(z_size*scale_factor)\n",
    "y_size = int(z_size*scale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_seg_img = tiff.imread(ground_seg_file)\n",
    "ground_seg_img = ground_seg_img.transpose((0, 3, 2, 1)) # t, x, y, z\n",
    "#ground_seg_img = ground_seg_img[:,0:x_size,0:y_size,0:z_size]\n",
    "print(ground_seg_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_seg_img is unlabelled, ground_seg_I0 is labelled, Gabys tracks\n",
    "\n",
    "detections = []\n",
    "#Labels = []\n",
    "\n",
    "for t in range(ground_seg_img.shape[0]):\n",
    "    frame = ground_seg_img[t]\n",
    "    coords, labels = get_P_list(frame, scales)\n",
    "\n",
    "    detections.append(coords)\n",
    "\n",
    "# tracking used ground_seg_img which is unlabelled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build graph in networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#103\n",
    "\n",
    "dfs_mot = []\n",
    "\n",
    "cov = covs[0]\n",
    "k = int(len(detections[0])/0.8)\n",
    "#k = int(len(detections[0]))\n",
    "#k = 100\n",
    "\n",
    "print(cov, k)\n",
    "\n",
    "spatial_threshold = spatial_thresholds[0]\n",
    "\n",
    "print(\"Building Graph\", cov, spatial_threshold, k)\n",
    "\n",
    "# Build graph\n",
    "G, d = build_graph(detections, int(k), spatial_threshold, cov)\n",
    "\n",
    "# !rm -rf solver/*.lgf\n",
    "# !rm -rf solver/*.txt\n",
    "\n",
    "print(\"Solving Flow\")\n",
    "convert_tracking_graph_to_lemon(G)\n",
    "\n",
    "subprocess.run([\n",
    "    \"g++\",\n",
    "    \"-o\", \"flow\",\n",
    "    \"cost_scaling.cc\",\n",
    "    \"-I\", \"/cis/home/gcoste1/.local/include\",\n",
    "    \"-L\", \"/cis/home/gcoste1/.local/lib\",\n",
    "    \"-lemon\"\n",
    "], cwd=\"solver\", check=True)\n",
    "\n",
    "# Run the compiled program with k as argument and redirect output to output.txt\n",
    "with open(\"solver/output.txt\", \"w\") as f:\n",
    "    subprocess.run([\"./flow\", str(k)], cwd=\"solver\", stdout=f, check=True)\n",
    "\n",
    "print(\"Reading Flow\")\n",
    "\n",
    "\n",
    "#assert(flowDict_filtered == flowDict_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Track Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"solver/output.txt\"\n",
    "flowCost_cpp, flowDict_cpp = parse_output_txt(filename)\n",
    "\n",
    "## Track stats\n",
    "\n",
    "# Example usage:\n",
    "source = (0, 0)\n",
    "sink = (len(detections)+1, 0)\n",
    "paths = extract_flow_paths(flowDict_cpp, source, sink)\n",
    "\n",
    "# Assuming you have the 'tracks' DataFrame with 'x', 'y', 'z', and 'track_id' columns.\n",
    "tracks = get_tracks(paths, detections)\n",
    "\n",
    "# Assuming 'tracks' is your DataFrame\n",
    "# Sort by 'track_id' and time\n",
    "tracks = tracks.sort_values(by=['track_id', 't'])\n",
    "\n",
    "# Compute the step-wise change in x, y, z\n",
    "tracks[['dx', 'dy', 'dz']] = tracks.groupby('track_id')[['x', 'y', 'z']].diff()\n",
    "\n",
    "# Compute Euclidean step distance\n",
    "tracks['step_distance'] = np.sqrt(tracks['dx']**2 + tracks['dy']**2 + tracks['dz']**2)\n",
    "\n",
    "# Remove NaN from first step per track (no previous point to diff)\n",
    "step_distances = tracks['step_distance'].dropna()\n",
    "\n",
    "# Compute the average step size across all steps\n",
    "average_step_size = step_distances.mean()\n",
    "\n",
    "print(\"Average step size (between consecutive time steps):\", average_step_size)\n",
    "\n",
    "\n",
    "## Recolor segmentation, it should look good (since physical looks good)\n",
    "\n",
    "\n",
    "# here we relabel ground_seg_I0\n",
    "\n",
    "seg_new = assign_track_ids_to_segmentation(ground_seg_img, tracks, scales)\n",
    "seg_new = seg_new.transpose((0, 3, 2, 1))  # t, x, y, z\n",
    "\n",
    "# # 4. Save the resulting array as a TIFF file\n",
    "tiff.imwrite('network_seg.tiff', seg_new.astype(np.uint16))\n",
    "\n",
    "# Convert 't' to datetime if it's not already\n",
    "tracks['t'] = pd.to_datetime(tracks['t'])\n",
    "\n",
    "# Get number of unique days each track appears\n",
    "track_days = tracks.groupby('track_id')['t'].nunique()\n",
    "\n",
    "# Calculate the mean number of days\n",
    "mean_days = track_days.mean()\n",
    "\n",
    "print(mean_days, average_step_size )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
